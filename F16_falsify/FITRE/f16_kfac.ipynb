{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tr_kfac_opt import KFACOptimizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-08 16:47:10,423\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '128.10.130.25',\n",
       " 'raylet_ip_address': '128.10.130.25',\n",
       " 'redis_address': '128.10.130.25:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-11-08_16-47-09_303808_6164/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-11-08_16-47-09_303808_6164/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-11-08_16-47-09_303808_6164',\n",
       " 'metrics_export_port': 50794}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init(num_cpus=50, num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CoRec\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reward(self, r):\n",
    "        if r <= -1:\n",
    "            r = 0\n",
    "        else:\n",
    "            r += 0.1\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_hyper(**kwargs): return kwargs\n",
    "\n",
    "config = ppo_hyper(\n",
    "    learning_rate=tune.loguniform(1e-6, 0.01),\n",
    "    n_steps=tune.choice([256, 512, 1024, 2048, 4096]),\n",
    "    batch_size=tune.choice([32, 64, 128, 256, 512, 1024, 2048]),\n",
    "    n_epochs=tune.randint(4, 32),\n",
    "    gamma=tune.uniform(0.95, 1),\n",
    "    gae_lambda=tune.uniform(0.95, 0.999),\n",
    "    clip_range=tune.uniform(0.01, 0.5),\n",
    "    clip_range_vf=None,\n",
    "    ent_coef=0.05,\n",
    "    vf_coef=tune.uniform(0.2, 0.8),\n",
    "    max_grad_norm=0.5,\n",
    "    use_sde=False,\n",
    "    sde_sample_freq=-1,\n",
    "    target_kl=None,\n",
    "    seed=1\n",
    ")\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_func(): return RewardWrapper(gym.make(\"F16GCAS-v3\"))\n",
    "\n",
    "env = make_vec_env(env_func, n_envs=4)\n",
    "model = PPO(MlpPolicy, env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only change is here\n",
    "see <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportCallback(sb3.common.callbacks.BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(ReportCallback, self).__init__(verbose)\n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        ep_rewards = [ep_info[\"r\"] for ep_info in self.model.ep_info_buffer]\n",
    "        ep_rew_mean = np.mean(ep_rewards)\n",
    "        tune.report(ep_rew = ep_rew_mean)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "def F16_train(config):\n",
    "    callback = ReportCallback()\n",
    "    model = PPO(MlpPolicy, env, verbose=1, **config)\n",
    "    model.policy.optimizer = KFACOptimizer(model.policy) # <-- Plugin here!\n",
    "    model.learn(total_timesteps=250000, callback=callback)\n",
    "    model.save(\"ppo_f16\")\n",
    "    \n",
    "analysis = tune.run(F16_train, config=config, num_samples=50, verbose=1, metric=\"ep_rew\", mode=\"max\", raise_on_failed_trial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "dfs = analysis.trial_dataframes\n",
    "for d in dfs.values():\n",
    "    ax = d.plot(\"training_iteration\", \"ep_rew\", ax=ax, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_config(metric=\"ep_rew\", mode=\"max\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
